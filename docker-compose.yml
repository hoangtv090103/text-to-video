services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: text-to-video-frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://server:8000
    depends_on:
      - server
    restart: unless-stopped

  server:
    build:
      context: ./server
      dockerfile: Dockerfile
    container_name: text-to-video-server
    ports:
      - "8000:8000"
    environment:
      - TTS_SERVICE_URL=http://chatterbox-tts:4123/v1/audio/speech
      - LLM_URL=${LLM_URL}
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_MODEL=${LLM_MODEL}
      - LOG_LEVEL=info
    depends_on:
      - chatterbox-tts
    restart: unless-stopped

  chatterbox-tts:
    build:
      context: ./chatterbox-tts-api
      dockerfile: docker/Dockerfile
    container_name: chatterbox-tts-api
    ports:
      - "4123:4123"
    environment:
      - PORT=4123
      - HOST=0.0.0.0
      - EXAGGERATION=0.5
      - CFG_WEIGHT=0.5
      - TEMPERATURE=0.7
      - MAX_CHUNK_LENGTH=250
      - MAX_TOTAL_LENGTH=2500
      - VOICE_SAMPLE_PATH=/app/voice-sample.mp3
      - DEVICE=cpu
      - MODEL_CACHE_DIR=/cache
      - VOICE_LIBRARY_DIR=/voices
      - MEMORY_CLEANUP_INTERVAL=3
      - CUDA_CACHE_CLEAR_INTERVAL=2
      - ENABLE_MEMORY_MONITORING=true
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
      - TORCH_NUM_THREADS=8
      - OPENBLAS_NUM_THREADS=8
      - VECLIB_MAXIMUM_THREADS=8
      - TORCH_CUDNN_V8_API_ENABLED=1
      - PYTORCH_ENABLE_MPS_FALLBACK=1
      - HF_HUB_DISABLE_TELEMETRY=1
      - TRANSFORMERS_OFFLINE=0
      - HF_HUB_CACHE=/cache/huggingface
    volumes:
      - ./voice-sample.mp3:/app/voice-sample.mp3:ro
      - chatterbox-models:/cache
      - chatterbox-voices:/voices
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4123/health"]
      interval: 45s
      timeout: 15s
      retries: 5
      start_period: 10m

volumes:
  chatterbox-models:
  chatterbox-voices:

networks:
  default:
