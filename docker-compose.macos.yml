services:
  redis:
    image: redis:7-alpine
    container_name: text-to-video-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 3s
      retries: 3
    restart: unless-stopped

  # Chatterbox TTS API - Optimized for macOS Docker
  chatterbox-tts:
    build:
      context: ./chatterbox-tts-api
      dockerfile: docker/Dockerfile.mps
    container_name: chatterbox-tts-api-macos
    ports:
      # Use an explicit host:container mapping to avoid accidental overrides from the host env
      - "4123:4123"

    # Optimize resource allocation for ARM64 macOS
    deploy:
      resources:
        limits:
          cpus: '0'  # Use all available CPUs
          memory: 16G  # Increased memory limit for TTS stability
        reservations:
          cpus: '6'
          memory: 8G  # Increased reservation for TTS

    # Docker Desktop optimizations
    platform: linux/arm64

    environment:
      # API Configuration
      # Use explicit values to ensure the service binds to all interfaces inside the container
      - PORT=4123
      - HOST=0.0.0.0

      # TTS Model Settings - optimized for CPU
      - EXAGGERATION=${EXAGGERATION:-0.5}
      - CFG_WEIGHT=${CFG_WEIGHT:-0.5}
      - TEMPERATURE=${TEMPERATURE:-0.7}  # Slightly lower for faster inference

      # Text Processing - optimized chunk sizes
      - MAX_CHUNK_LENGTH=${MAX_CHUNK_LENGTH:-250}  # Smaller chunks for CPU
      - MAX_TOTAL_LENGTH=${MAX_TOTAL_LENGTH:-2500}

      # Device and paths
      - VOICE_SAMPLE_PATH=/app/voice-sample.mp3
      - DEVICE=cpu  # Force CPU since MPS is not available in Docker
      - MODEL_CACHE_DIR=/cache
      - VOICE_LIBRARY_DIR=/voices

      # Memory management - aggressive for CPU
      - MEMORY_CLEANUP_INTERVAL=${MEMORY_CLEANUP_INTERVAL:-1}
      - CUDA_CACHE_CLEAR_INTERVAL=${CUDA_CACHE_CLEAR_INTERVAL:-1}
      - ENABLE_MEMORY_MONITORING=${ENABLE_MEMORY_MONITORING:-true}
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256,expandable_segments:True
      
      # Additional memory optimizations
      - PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
      - TOKENIZERS_PARALLELISM=false
      - PYTORCH_MEMORY_EFFICIENT_ATTENTION=1

      # CPU optimization environment variables
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
      - TORCH_NUM_THREADS=8
      - OPENBLAS_NUM_THREADS=8
      - VECLIB_MAXIMUM_THREADS=8

      # PyTorch optimizations
      - TORCH_CUDNN_V8_API_ENABLED=1
      - PYTORCH_ENABLE_MPS_FALLBACK=1

      # HuggingFace optimizations
      - HF_HUB_DISABLE_TELEMETRY=1
      - TRANSFORMERS_OFFLINE=0
      - HF_HUB_CACHE=/cache/huggingface

    volumes:
      # Voice sample (from parent directory)
      - ${VOICE_SAMPLE_HOST_PATH:-./voice-sample.mp3}:/app/voice-sample.mp3:ro

      # Persistent model cache
      - chatterbox-models:/cache

      # Persistent voice library
      - chatterbox-voices:/voices

      # Optional: Custom voice samples directory
      - ${VOICE_SAMPLES_DIR:-./chatterbox-tts-api//voice-samples}:/app/voice-samples:ro

    # Use tmpfs for temporary files to improve I/O performance
    tmpfs:
      - /tmp:size=2G,noexec,nosuid,nodev
      - /var/tmp:size=1G,noexec,nosuid,nodev

    # Optimize shared memory for better performance
    shm_size: 4g

    # Use host network mode for better performance (optional)
    # network_mode: host  # Uncomment if you need maximum performance

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-4123}/health"]
      interval: 45s
      timeout: 15s
      retries: 5
      start_period: 10m  # Give it more time to initialize

  server:
    build: ./server
    container_name: text-to-video-server
    ports:
      - "8000:8000"
    environment:
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - TTS_SERVICE_URL=http://chatterbox-tts:4123/v1/audio/speech
      - PRESENTON_BASE_URL=http://presenton:80
      - LLM_URL=${LLM_URL}
      - LLM_API_KEY=${LLM_API_KEY}
      - LLM_MODEL=${LLM_MODEL}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-3.5-turbo}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - GOOGLE_MODEL=${GOOGLE_MODEL:-gemini-pro}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ANTHROPIC_MODEL=${ANTHROPIC_MODEL:-claude-3-sonnet-20240229}
      - HUGGINGFACE_MODEL=${HUGGINGFACE_MODEL:-google/flan-t5-base}
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
      - LOCAL_MODEL_PATH=${LOCAL_MODEL_PATH:-llama2}
      - LOCAL_MODEL_TYPE=${LOCAL_MODEL_TYPE:-llama}
    volumes:
      - ./server/data:/app/data
    depends_on:
      - redis
      - chatterbox-tts
      - presenton
    restart: unless-stopped
    user: "0"

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: text-to-video-frontend
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
    depends_on:
      - server
    restart: unless-stopped

  presenton:
    image: ghcr.io/presenton/presenton:latest
    container_name: presenton
    user: "0"
    ports:
      - "${PRESENTON_PORT:-5001}:80"
    environment:
      # Core configuration
      - APP_DATA_DIRECTORY=/app_data
      - TEMP_DIRECTORY=/tmp/presenton
      - CAN_CHANGE_KEYS=${PRESENTON_CAN_CHANGE_KEYS:-true}

      # LLM Provider Configuration (choose one)
      - LLM=${PRESENTON_LLM:-google}

      - PRESENTON_CUSTOM_LLM_URL=${PRESENTON_CUSTOM_LLM_URL}
      - PRESENTON_CUSTOM_MODEL=${PRESENTON_CUSTOM_MODEL}
      - PRESENTON_CUSTOM_LLM_API_KEY=${PRESENTON_CUSTOM_LLM_API_KEY}
      - PRESENTON_PEXELS_API_KEY=${PRESENTON_PEXELS_API_KEY}

      # OpenAI Configuration
      - OPENAI_API_KEY=${PRESENTON_OPENAI_API_KEY}
      - OPENAI_MODEL=${PRESENTON_OPENAI_MODEL:-gpt-4-turbo}

      # Google Configuration
      - GOOGLE_API_KEY=${PRESENTON_CUSTOM_LLM_API_KEY}
      - GOOGLE_MODEL=${PRESENTON_GOOGLE_MODEL:-gemini-2.0-flash}
      - WEB_GROUNDING=${PRESENTON_WEB_GROUNDING:-false}

      # Anthropic Configuration
      - ANTHROPIC_API_KEY=${PRESENTON_ANTHROPIC_API_KEY}
      - ANTHROPIC_MODEL=${PRESENTON_ANTHROPIC_MODEL:-claude-3-5-sonnet-20241022}

      # Ollama Configuration (for local models)
      - OLLAMA_URL=${PRESENTON_OLLAMA_URL:-http://localhost:11434}
      - OLLAMA_MODEL=${PRESENTON_OLLAMA_MODEL:-llama3.2:3b}

      # Custom LLM Configuration (for OpenAI-compatible APIs)
      - CUSTOM_LLM_URL=${PRESENTON_CUSTOM_LLM_URL}
      - CUSTOM_LLM_API_KEY=${PRESENTON_CUSTOM_LLM_API_KEY}
      - CUSTOM_MODEL=${PRESENTON_CUSTOM_MODEL}

      # Image Provider Configuration
      - IMAGE_PROVIDER=${PRESENTON_IMAGE_PROVIDER:-dall-e-3}
      - PEXELS_API_KEY=${PRESENTON_PEXELS_API_KEY}
      - PIXABAY_API_KEY=${PRESENTON_PIXABAY_API_KEY}

      # Advanced Features
      - TOOL_CALLS=${PRESENTON_TOOL_CALLS:-false}
      - DISABLE_THINKING=${PRESENTON_DISABLE_THINKING:-false}
      - EXTENDED_REASONING=${PRESENTON_EXTENDED_REASONING:-false}
      - USE_CUSTOM_URL=${PRESENTON_USE_CUSTOM_URL:-false}
    volumes:
      - presenton_data:/app_data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  redis_data:
  chatterbox-models:
  chatterbox-voices:
  chatterbox-models-macos:
  chatterbox-voices-macos:
  presenton_data:

networks:
  default:
    driver: bridge
    driver_opts:
      com.docker.network.driver.mtu: 1500
